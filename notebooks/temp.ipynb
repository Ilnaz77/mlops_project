{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    'client_kwargs': {\n",
    "        'endpoint_url': \"https://storage.yandexcloud.net\",\n",
    "    }\n",
    "}\n",
    "\n",
    "data_train = pd.read_csv(\"s3://zoomcamp-mlops/data/twitter_training.csv\", storage_options=options, header=None)\n",
    "data_val = pd.read_csv(\"s3://zoomcamp-mlops/data/twitter_validation.csv\", storage_options=options, header=None)\n",
    "data = pd.concat([data_train, data_val])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "data.drop([0,1], axis=1, inplace=True)\n",
    "data.columns = ['sentiment','text']\n",
    "data.sentiment = data.sentiment.map({\"Neutral\":0, \"Irrelevant\":0 ,\"Positive\":1,\"Negative\":2})\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       sentiment                                               text\n0              1  im getting on borderlands and i will murder yo...\n1              1  I am coming to the borders and I will kill you...\n2              1  im getting on borderlands and i will kill you ...\n3              1  im coming on borderlands and i will murder you...\n4              1  im getting on borderlands 2 and i will murder ...\n...          ...                                                ...\n72134          0  ♥️ Suikoden 2\\n1️⃣ Alex Kidd in Miracle World\\...\n72135          1  Thank you to Matching funds Home Depot RW paym...\n72136          0  Late night stream with the boys! Come watch so...\n72137          0  ⭐️ Toronto is the arts and culture capital of ...\n72138          0  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...\n\n[72139 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>im getting on borderlands and i will murder yo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>I am coming to the borders and I will kill you...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>im getting on borderlands and i will kill you ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>im coming on borderlands and i will murder you...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>im getting on borderlands 2 and i will murder ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>72134</th>\n      <td>0</td>\n      <td>♥️ Suikoden 2\\n1️⃣ Alex Kidd in Miracle World\\...</td>\n    </tr>\n    <tr>\n      <th>72135</th>\n      <td>1</td>\n      <td>Thank you to Matching funds Home Depot RW paym...</td>\n    </tr>\n    <tr>\n      <th>72136</th>\n      <td>0</td>\n      <td>Late night stream with the boys! Come watch so...</td>\n    </tr>\n    <tr>\n      <th>72137</th>\n      <td>0</td>\n      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n    </tr>\n    <tr>\n      <th>72138</th>\n      <td>0</td>\n      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n    </tr>\n  </tbody>\n</table>\n<p>72139 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    replace_list = {r\"i'm\": 'i am',\n",
    "                    r\"'re\": ' are',\n",
    "                    r\"let’s\": 'let us',\n",
    "                    r\"'s\":  ' is',\n",
    "                    r\"'ve\": ' have',\n",
    "                    r\"can't\": 'can not',\n",
    "                    r\"cannot\": 'can not',\n",
    "                    r\"shan’t\": 'shall not',\n",
    "                    r\"n't\": ' not',\n",
    "                    r\"'d\": ' would',\n",
    "                    r\"'ll\": ' will',\n",
    "                    r\"'scuse\": 'excuse',\n",
    "                    ',': ' ,',\n",
    "                    '.': ' .',\n",
    "                    '!': ' !',\n",
    "                    '?': ' ?',\n",
    "                    '\\s+': ' '}\n",
    "    text = text.lower()\n",
    "    for s in replace_list:\n",
    "        text = text.replace(s, replace_list[s])\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "data[\"text\"] = data[\"text\"].map(clean_text)\n",
    "data = data[data[\"text\"] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "data.to_parquet(\"s3://zoomcamp-mlops/data/origin/data.pq\", storage_options=options)\n",
    "data.to_parquet(\"s3://zoomcamp-mlops/data/origin/old/data.pq\", storage_options=options)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(data, test_size=0.1, stratify=data[\"sentiment\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, test = train_test_split(val, test_size=0.2, stratify=val[\"sentiment\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       sentiment                                               text\n35961          0  we still need the best women is clothes @ play...\n43160          2                                                 my\n55760          2  @rainbow6game why the world is theme party sti...\n1406           1  gonna get lost in borderlands 2 and batman: da...\n65380          2  @cyberpunkgame maybe you announce release date...\n...          ...                                                ...\n6544           0  @ eljeeeey thank you for participating in summ...\n11670          1  that i do miss basket ball more than anything ...\n11848          1  this is what seems the entire world needs righ...\n17343          2                           nice looking mini router\n45864          1  some milestones and anniversaries here at the ...\n\n[5757 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>35961</th>\n      <td>0</td>\n      <td>we still need the best women is clothes @ play...</td>\n    </tr>\n    <tr>\n      <th>43160</th>\n      <td>2</td>\n      <td>my</td>\n    </tr>\n    <tr>\n      <th>55760</th>\n      <td>2</td>\n      <td>@rainbow6game why the world is theme party sti...</td>\n    </tr>\n    <tr>\n      <th>1406</th>\n      <td>1</td>\n      <td>gonna get lost in borderlands 2 and batman: da...</td>\n    </tr>\n    <tr>\n      <th>65380</th>\n      <td>2</td>\n      <td>@cyberpunkgame maybe you announce release date...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6544</th>\n      <td>0</td>\n      <td>@ eljeeeey thank you for participating in summ...</td>\n    </tr>\n    <tr>\n      <th>11670</th>\n      <td>1</td>\n      <td>that i do miss basket ball more than anything ...</td>\n    </tr>\n    <tr>\n      <th>11848</th>\n      <td>1</td>\n      <td>this is what seems the entire world needs righ...</td>\n    </tr>\n    <tr>\n      <th>17343</th>\n      <td>2</td>\n      <td>nice looking mini router</td>\n    </tr>\n    <tr>\n      <th>45864</th>\n      <td>1</td>\n      <td>some milestones and anniversaries here at the ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5757 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.to_parquet(\"s3://zoomcamp-mlops/data/val.pq\", storage_options=options)\n",
    "train.to_parquet(\"s3://zoomcamp-mlops/data/train.pq\", storage_options=options)\n",
    "test.to_parquet(\"s3://zoomcamp-mlops/data/test.pq\", storage_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "options = {\n",
    "    'client_kwargs': {\n",
    "        'endpoint_url': \"https://storage.yandexcloud.net\",\n",
    "    }\n",
    "}\n",
    "\n",
    "val = pd.read_parquet(\"s3://zoomcamp-mlops/data/val.pq\", storage_options=options)\n",
    "train = pd.read_parquet(\"s3://zoomcamp-mlops/data/train.pq\", storage_options=options)\n",
    "test = pd.read_parquet(\"s3://zoomcamp-mlops/data/test.pq\", storage_options=options)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "val.to_parquet(\"s3://zoomcamp-mlops/data/temp/val.pq\", storage_options=options)\n",
    "train.to_parquet(\"s3://zoomcamp-mlops/data/temp/train.pq\", storage_options=options)\n",
    "test.to_parquet(\"s3://zoomcamp-mlops/data/temp/test.pq\", storage_options=options)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "val_curr = pd.read_parquet(\"s3://zoomcamp-mlops/data/val.pq\", storage_options=options)\n",
    "val_old = pd.read_parquet(\"s3://zoomcamp-mlops/data/temp/val.pq\", storage_options=options)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.util.hash_pandas_object(val_curr).sum() == pd.util.hash_pandas_object(val_old).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90381ea4366b0d59eab9e862c070b8e21eb49d15f4a702a75e1d74a11d6f4d5d\n",
      "90381ea4366b0d59eab9e862c070b8e21eb49d15f4a702a75e1d74a11d6f4d5d\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "print(hashlib.sha256(val_curr.to_json().encode()).hexdigest())\n",
    "print(hashlib.sha256(val_old.to_json().encode()).hexdigest())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "from tokenizers import Tokenizer, decoders\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from torch import LongTensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "from typing import Any\n",
    "from pickle import dump\n",
    "import pandas as pd\n",
    "\n",
    "def _s3() -> s3fs.S3FileSystem:\n",
    "    return s3fs.S3FileSystem(\n",
    "                anon=False,\n",
    "                key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "                endpoint_url=os.environ[\"AWS_ENDPOINT_URL\"])\n",
    "    \n",
    "def write_pickle_to_s3(obj: Any, s3_path: str) -> None:\n",
    "    s3 = _s3()\n",
    "    dump(obj, s3.open(s3_path, 'wb'))\n",
    "\n",
    "def read_pickle_from_s3(s3_path: str) -> Any:\n",
    "    s3 = _s3()\n",
    "    return pickle.load(s3.open(s3_path))\n",
    "\n",
    "def read_parquet_s3(s3_path: str) -> pd.DataFrame:\n",
    "    options = {\n",
    "                'client_kwargs': {\n",
    "                    'endpoint_url': os.environ[\"AWS_ENDPOINT_URL\"],\n",
    "                }\n",
    "            }\n",
    "\n",
    "    return pd.read_parquet(s3_path, storage_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyBpeSubWords:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_freq: int = 5,\n",
    "        batch_size: int = 5000,\n",
    "        path_to_tokenizer: str = \"./\",\n",
    "    ):\n",
    "\n",
    "        self.stoi = dict()\n",
    "        self.n_freq = n_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.path_to_save_tokenizer = path_to_tokenizer + \"vocab.pkl\"\n",
    "        self.special_tokens = [\"<PAD>\", \"<UNK>\"]\n",
    "        \n",
    "        self.tokenizer = Tokenizer(WordLevel(unk_token=\"<UNK>\"))\n",
    "        self.tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.tokenizer.get_vocab_size()\n",
    "\n",
    "    def save_vocab(self):\n",
    "        write_pickle_to_s3(self.__dict__, self.path_to_save_tokenizer)\n",
    "\n",
    "    def read_vocab(self):\n",
    "        self.__dict__ = read_pickle_from_s3(self.path_to_save_tokenizer)\n",
    "\n",
    "    def _get_training_corpus(self, sents: List[str]) -> List[str]:\n",
    "        len_sents = len(sents)\n",
    "        for i in range(0, len_sents, self.batch_size):\n",
    "            yield sents[i : i + self.batch_size]\n",
    "\n",
    "    def build_vocabulary(self, sentences_list: List[str]):\n",
    "        trainer = WordLevelTrainer(\n",
    "            special_tokens=self.special_tokens,\n",
    "            min_frequency=self.n_freq,\n",
    "        )\n",
    "        self.tokenizer.train_from_iterator(self._get_training_corpus(sentences_list), trainer=trainer)\n",
    "        self.stoi = self.tokenizer.get_vocab()\n",
    "        self.save_vocab()\n",
    "\n",
    "    def numericalize(self, text: str) -> List[int]:\n",
    "        tokenized: List[int] = []\n",
    "        # cad add sos token in the beginning\n",
    "        encoding = self.tokenizer.encode(text)\n",
    "        return encoding.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, pad_idx: int = 0):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        words = [torch.tensor(item[\"words\"]) for item in batch]\n",
    "        labels = torch.tensor([item[\"label\"] for item in batch])\n",
    "        \n",
    "        # idxes to sort by sent len\n",
    "        seq_lengths, perm_idx = LongTensor(list(map(len, words))).sort(0, descending=True)\n",
    "\n",
    "        # sort by len sentence in desc\n",
    "        words = [words[i] for i in perm_idx]\n",
    "\n",
    "        # pad words: [batch_size, max_sentence_len_in_batch]\n",
    "        padd_words = pad_sequence(words, padding_value=self.pad_idx, batch_first=True)\n",
    "\n",
    "        return {\"words\": padd_words, \n",
    "                \"label\": labels,\n",
    "                \"seq_lengths\": seq_lengths}\n",
    "\n",
    "class QueryDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        vocab_words: VocabularyWords,\n",
    "    ):\n",
    "        self.data: pd.DataFrame = read_parquet_s3(file_path)\n",
    "        self.vocab_words = vocab_words\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
    "        line: str = self.data.iloc[index]\n",
    "        label: int = line[\"sentiment\"]\n",
    "        text: str = line[\"text\"]\n",
    "        numericalized_word: List[int] = self.vocab_words.numericalize(text)\n",
    "\n",
    "        return {\"text\": numericalized_word, \n",
    "                \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = VocabularyBpeSubWords(path_to_tokenizer=\"s3://zoomcamp-mlops/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = QueryDataset(file_path=\"s3://zoomcamp-mlops/data/twitter_validation.pq\", vocab_words=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://zoomcamp-mlops/data/vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "v.build_vocabulary(d.data.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [32, 40, 32, 3, 331, 1, 1, 218, 387, 10, 1, 4, 500, 1, 1, 1],\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
